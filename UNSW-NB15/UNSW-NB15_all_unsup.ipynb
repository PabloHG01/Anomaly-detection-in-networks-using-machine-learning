{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNSW-NB15\n",
    "Implementación de los IDS para el conjunto total de datos usando algoritmos de aprendizaje no supervisado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File              ML algorithm                 Accuracy        Precision       Recall          F1-score        Time           \n",
      "all_data_preproc  K-Means                      0.85            0.44            0.48            0.46            5.7211         \n",
      "mission accomplished!\n",
      "Total operation time: =  64.37577724456787 seconds\n"
     ]
    }
   ],
   "source": [
    "## ALL_DATA: 7 LABELS - K-MEDIAS\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.cluster import completeness_score\n",
    "\n",
    "#Añadimos nuevos algoritmos de clasificación (no supervisado):\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib as mpl\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler  #Escalar\n",
    "from sklearn.preprocessing import StandardScaler #Estandarizar\n",
    "from sklearn.preprocessing import Normalizer #Normalizar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=50)\n",
    "#%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "import warnings\n",
    "import math\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "result=\"./results/results_2.csv\" #a CSV file is named in which the results are saved.\n",
    "csv_files=[\"all_data_preproc.csv\"]# CSV files names: #The names of the dataset files (csv_files).\n",
    "path=\"\"\n",
    "repetition=10\n",
    "\n",
    "# the features to be used for each attack type is defined in a dictionary(features).\n",
    "# the first 4 of the features created by the file \"04_1_feature_selection_for_attack_files.py\" are used here.\n",
    "features={  'all_data_preproc': [\"dsport\", \"sbytes\", \"sport\", \"Sload\", \"dbytes\", \"Spkts\", \"dstip\", \"Label\"]}\n",
    "\n",
    "seconds=time.time()#time stamp for all processing time\n",
    "\n",
    "\n",
    "print ('%-17s %-27s  %-15s %-15s %-15s %-15s %-15s' % (\"File\",\"ML algorithm\",\"Accuracy\",\"Precision\", \"Recall\" , \"F1-score\",\"Time\"))# print output header\n",
    "\n",
    "\n",
    "for j in csv_files: #this loop runs on the list containing the filenames.Operations are repeated for all attack files\n",
    "    a=[]\n",
    "    \n",
    "    feature_list=list(features[j[0:-4]])\n",
    "    df=pd.read_csv(path+j,usecols=feature_list)#read an attack file.\n",
    "    df=df.fillna(0)\n",
    "    attack_or_not=[]\n",
    "    for i in df[\"Label\"]: #it changes the normal label to \"1\" and the attack tag to \"0\" for use in the machine learning algorithm\n",
    "        \n",
    "        if i ==\"BENIGN\":\n",
    "            attack_or_not.append(1)\n",
    "        else:\n",
    "            attack_or_not.append(0)\n",
    "    df[\"Label\"]=attack_or_not\n",
    "\n",
    "    \n",
    "    y = df[\"Label\"] #this section separates the label and the data into two separate pieces, as Label=y Data=X \n",
    "    del df[\"Label\"]\n",
    "    feature_list.remove('Label')\n",
    "    X = df[feature_list]\n",
    "    \n",
    "    precision=[]\n",
    "    recall=[]\n",
    "    f1=[]\n",
    "    accuracy=[]\n",
    "    t_time=[]\n",
    "    \n",
    "    for i in range(repetition): # This loop allows cross-validation and machine learning algorithm to be repeated 10 times\n",
    "        #for i in range(repetition): # This loop allows cross-validation and machine learning algorithm to be repeated 10 times\n",
    "        second=time.time()#time stamp for processing time\n",
    "\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "\n",
    "        # cross-validation\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y,#  data (X) and labels (y) are divided into 2 parts to be sent to the machine learning algorithm (80% train,%20 test). \n",
    "            test_size = 0.20, random_state = repetition)#  So, in total there are 4 tracks: training data(X_train), training tag (y_train), test data(X_test) and test tag(y_test).\n",
    "\n",
    "        #print(\"  Numero de clusters: %d\\n\" % (2))\n",
    "        km = KMeans(init='random', n_clusters=2, random_state=333)\n",
    "        km.fit(X_train)     \n",
    "        predict = km.predict(X_test)\n",
    "\n",
    "        #Para poder identificar qué cluster corresponde\n",
    "        sum0 = sum(1 for p in predict if p == 0)\n",
    "        sum1 = sum(1 for p in predict if p == 1)\n",
    "        if(sum0 > sum1):\n",
    "            predict = predict + 1\n",
    "            predict = np.where(predict == 2, 0, predict)\n",
    "\n",
    "\n",
    "        f_1=f1_score(y_test, predict, average='macro')\n",
    "        pr=precision_score(y_test, predict, average='macro')\n",
    "        rc=recall_score(y_test, predict, average='macro')\n",
    "        score = metrics.accuracy_score(y_test, predict)\n",
    "\n",
    "        precision.append(float(pr))\n",
    "        recall.append(float(rc))\n",
    "        f1.append(float(f_1))\n",
    "        accuracy.append(score)\n",
    "        t_time.append(float((time.time()-second)))\n",
    "\n",
    "    print ('%-17s %-27s  %-15s %-15s %-15s %-15s %-15s' % (j[0:-4],\"K-Means\",str(round(np.mean(accuracy),2)),str(round(np.mean(precision),2)), \n",
    "            str(round(np.mean(recall),2)),str(round(np.mean(f1),2)),str(round(np.mean(t_time),4))))#the result of the ten repetitions is printed on the screen.\n",
    "\n",
    "\n",
    "\n",
    "    #print(\"\\n------------------------------------------------------------------------------------------------------\\n\\n\")\n",
    "    \n",
    "print(\"mission accomplished!\")\n",
    "print(\"Total operation time: = \",time.time()- seconds ,\"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File              ML algorithm                 Accuracy        Precision       Recall          F1-score        Time           \n",
      "all_data_preproc  K-Medoids                    0.76            0.59            0.65            0.6             3.2946         \n",
      "mission accomplished!\n",
      "Total operation time: =  40.2995171546936 seconds\n"
     ]
    }
   ],
   "source": [
    "## ALL_DATA: 7 LABELS - K-MEDOIDS\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.cluster import completeness_score\n",
    "\n",
    "#Añadimos nuevos algoritmos de clasificación (no supervisado):\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib as mpl\n",
    "import math\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib as mpl\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler  #Escalar\n",
    "from sklearn.preprocessing import StandardScaler #Estandarizar\n",
    "from sklearn.preprocessing import Normalizer #Normalizar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=50)\n",
    "#%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "import warnings\n",
    "import math\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "result=\"./results/results_2.csv\" #a CSV file is named in which the results are saved.\n",
    "csv_files=[\"all_data_preproc.csv\"]# CSV files names: #The names of the dataset files (csv_files).\n",
    "path=\"\"\n",
    "repetition=10\n",
    "\n",
    "# the features to be used for each attack type is defined in a dictionary(features).\n",
    "# the first 4 of the features created by the file \"04_1_feature_selection_for_attack_files.py\" are used here.\n",
    "features={  'all_data_preproc': [\"dsport\", \"sbytes\", \"sport\", \"Sload\", \"dbytes\", \"Spkts\", \"dstip\", \"Label\"]}\n",
    "\n",
    "seconds=time.time()#time stamp for all processing time\n",
    "\n",
    "\n",
    "print ('%-17s %-27s  %-15s %-15s %-15s %-15s %-15s' % (\"File\",\"ML algorithm\",\"Accuracy\",\"Precision\", \"Recall\" , \"F1-score\",\"Time\"))# print output header\n",
    "\n",
    "\n",
    "for j in csv_files: #this loop runs on the list containing the filenames.Operations are repeated for all attack files\n",
    "    a=[]\n",
    "    \n",
    "    feature_list=list(features[j[0:-4]])\n",
    "    df=pd.read_csv(path+j,usecols=feature_list)#read an attack file.\n",
    "    df=df.fillna(0)\n",
    "    numRows = min(df.shape[0]-1, 10000) #Para que no explote el consumo de memoria\n",
    "    df = df.sample(numRows)\n",
    "    attack_or_not=[]\n",
    "    for i in df[\"Label\"]: #it changes the normal label to \"1\" and the attack tag to \"0\" for use in the machine learning algorithm\n",
    "        \n",
    "        if i ==\"BENIGN\":\n",
    "            attack_or_not.append(1)\n",
    "        else:\n",
    "            attack_or_not.append(0)\n",
    "    df[\"Label\"]=attack_or_not\n",
    "\n",
    "    \n",
    "    y = df[\"Label\"] #this section separates the label and the data into two separate pieces, as Label=y Data=X \n",
    "    del df[\"Label\"]\n",
    "    feature_list.remove('Label')\n",
    "    X = df[feature_list]\n",
    "    \n",
    "    precision=[]\n",
    "    recall=[]\n",
    "    f1=[]\n",
    "    accuracy=[]\n",
    "    t_time=[]\n",
    "    \n",
    "    for i in range(repetition): # This loop allows cross-validation and machine learning algorithm to be repeated 10 times\n",
    "        #for i in range(repetition): # This loop allows cross-validation and machine learning algorithm to be repeated 10 times\n",
    "        second=time.time()#time stamp for processing time\n",
    "\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "        \n",
    "        # cross-validation\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y,#  data (X) and labels (y) are divided into 2 parts to be sent to the machine learning algorithm (80% train,%20 test). \n",
    "            test_size = 0.20, random_state = repetition)#  So, in total there are 4 tracks: training data(X_train), training tag (y_train), test data(X_test) and test tag(y_test).\n",
    "\n",
    "        #print(\"  Numero de clusters: %d\\n\" % (2))\n",
    "        km = KMedoids(init='random', n_clusters=2, random_state=333)\n",
    "        km.fit(X_train)     \n",
    "        predict = km.predict(X_test)\n",
    "\n",
    "        #Para poder identificar qué cluster corresponde\n",
    "        sum0 = sum(1 for p in predict if p == 0)\n",
    "        sum1 = sum(1 for p in predict if p == 1)\n",
    "        if(sum0 > sum1):\n",
    "            predict = predict + 1\n",
    "            predict = np.where(predict == 2, 0, predict)\n",
    "\n",
    "\n",
    "        f_1=f1_score(y_test, predict, average='macro')\n",
    "        pr=precision_score(y_test, predict, average='macro')\n",
    "        rc=recall_score(y_test, predict, average='macro')\n",
    "        score = metrics.accuracy_score(y_test, predict)\n",
    "\n",
    "        precision.append(float(pr))\n",
    "        recall.append(float(rc))\n",
    "        f1.append(float(f_1))\n",
    "        accuracy.append(score)\n",
    "        t_time.append(float((time.time()-second)))\n",
    "\n",
    "    print ('%-17s %-27s  %-15s %-15s %-15s %-15s %-15s' % (j[0:-4],\"K-Medoids\",str(round(np.mean(accuracy),2)),str(round(np.mean(precision),2)), \n",
    "            str(round(np.mean(recall),2)),str(round(np.mean(f1),2)),str(round(np.mean(t_time),4))))#the result of the ten repetitions is printed on the screen.\n",
    "\n",
    "    #print(\"\\n------------------------------------------------------------------------------------------------------\\n\\n\")\n",
    "    \n",
    "print(\"mission accomplished!\")\n",
    "print(\"Total operation time: = \",time.time()- seconds ,\"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File              ML algorithm                 Accuracy        Precision       Recall          F1-score        Time           \n",
      "all_data_preproc  Isolation Forest             0.67            0.53            0.55            0.51            7.7038         \n",
      "mission accomplished!\n",
      "Total operation time: =  86.37373161315918 seconds\n"
     ]
    }
   ],
   "source": [
    "## ALL_DATA: 7 LABELS - Isolation Forest\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.cluster import completeness_score\n",
    "\n",
    "#Añadimos nuevos algoritmos de clasificación (no supervisado):\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib as mpl\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler  #Escalar\n",
    "from sklearn.preprocessing import StandardScaler #Estandarizar\n",
    "from sklearn.preprocessing import Normalizer #Normalizar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=50)\n",
    "#%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "import warnings\n",
    "import math\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "result=\"./results/results_2.csv\" #a CSV file is named in which the results are saved.\n",
    "csv_files=[\"all_data_preproc.csv\"]# CSV files names: #The names of the dataset files (csv_files).\n",
    "path=\"\"\n",
    "repetition=10\n",
    "\n",
    "ml_list={\n",
    "\"Isolation Forest\": IsolationForest(random_state=123, n_estimators = 30, contamination = 0.3, max_samples = \"auto\") #Sabemos que el 30% de los datos corresponden con ataques => contamination=0.3\n",
    "}\n",
    "\n",
    "# the features to be used for each attack type is defined in a dictionary(features).\n",
    "# the first 4 of the features created by the file \"04_1_feature_selection_for_attack_files.py\" are used here.\n",
    "features={  'all_data_preproc': [\"dsport\", \"sbytes\", \"sport\", \"Sload\", \"dbytes\", \"Spkts\", \"dstip\", \"Label\"]}\n",
    "\n",
    "seconds=time.time()#time stamp for all processing time\n",
    "\n",
    "\n",
    "print ('%-17s %-27s  %-15s %-15s %-15s %-15s %-15s' % (\"File\",\"ML algorithm\",\"Accuracy\",\"Precision\", \"Recall\" , \"F1-score\",\"Time\"))# print output header\n",
    "\n",
    "\n",
    "for j in csv_files: #this loop runs on the list containing the filenames.Operations are repeated for all attack files\n",
    "    a=[]\n",
    "    \n",
    "    feature_list=list(features[j[0:-4]])\n",
    "    df=pd.read_csv(path+j,usecols=feature_list)#read an attack file.\n",
    "    df=df.fillna(0)\n",
    "    attack_or_not=[]\n",
    "    for i in df[\"Label\"]: #it changes the normal label to \"1\" and the attack tag to \"0\" for use in the machine learning algorithm\n",
    "        \n",
    "        if i ==\"BENIGN\":\n",
    "            attack_or_not.append(1)\n",
    "        else:\n",
    "            attack_or_not.append(0)\n",
    "    df[\"Label\"]=attack_or_not\n",
    "\n",
    "    \n",
    "    y = df[\"Label\"] #this section separates the label and the data into two separate pieces, as Label=y Data=X \n",
    "    del df[\"Label\"]\n",
    "    feature_list.remove('Label')\n",
    "    X = df[feature_list]\n",
    "    \n",
    "    for ii in ml_list:\n",
    "        precision=[]\n",
    "        recall=[]\n",
    "        f1=[]\n",
    "        accuracy=[]\n",
    "        t_time=[]\n",
    "        \n",
    "        for i in range(repetition): # This loop allows cross-validation and machine learning algorithm to be repeated 10 times\n",
    "            #for i in range(repetition): # This loop allows cross-validation and machine learning algorithm to be repeated 10 times\n",
    "            second=time.time()#time stamp for processing time\n",
    "\n",
    "            # cross-validation\n",
    "            scaler = StandardScaler()\n",
    "            X = scaler.fit_transform(X)\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y,#  data (X) and labels (y) are divided into 2 parts to be sent to the machine learning algorithm (80% train,%20 test). \n",
    "                test_size = 0.20, random_state = repetition)#  So, in total there are 4 tracks: training data(X_train), training tag (y_train), test data(X_test) and test tag(y_test).\n",
    "\n",
    "            #scaler = MinMaxScaler()\n",
    "            #scaler = StandardScaler()\n",
    "            #scaler = Normalizer()\n",
    "            #X_train = scaler.fit_transform(X_train)\n",
    "            #X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "            clf = ml_list[ii]#choose algorithm from ml_list dictionary                                                                          \n",
    "            clf.fit(X_train)\n",
    "            predict =clf.predict(X_test)\n",
    "\n",
    "            predict = np.where(predict == -1, 0, predict) #El resultado de isolation forest son 1 para los normales y -1 para anomalías\n",
    "\n",
    "            #makes \"classification report\" and assigns the precision, f-measure, and recall values.s.    \n",
    "            f_1=f1_score(y_test, predict, average='macro')\n",
    "            pr=precision_score(y_test, predict, average='macro')\n",
    "            rc=recall_score(y_test, predict, average='macro')\n",
    "            score = metrics.accuracy_score(y_test, predict)\n",
    "\n",
    "            precision.append(float(pr))\n",
    "            recall.append(float(rc))\n",
    "            f1.append(float(f_1))\n",
    "            accuracy.append(score)\n",
    "            t_time.append(float((time.time()-second)))\n",
    "\n",
    "        print ('%-17s %-27s  %-15s %-15s %-15s %-15s %-15s' % (j[0:-4],ii,str(round(np.mean(accuracy),2)),str(round(np.mean(precision),2)), \n",
    "                str(round(np.mean(recall),2)),str(round(np.mean(f1),2)),str(round(np.mean(t_time),4))))#the result of the ten repetitions is printed on the screen.\n",
    "\n",
    "\n",
    "\n",
    "    #print(\"\\n------------------------------------------------------------------------------------------------------\\n\\n\")\n",
    "    \n",
    "print(\"mission accomplished!\")\n",
    "print(\"Total operation time: = \",time.time()- seconds ,\"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File              ML algorithm                 Accuracy        Precision       Recall          F1-score        Time           \n"
     ]
    }
   ],
   "source": [
    "## ALL_DATA: 7 LABELS - Local Outlier Factor\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.cluster import completeness_score\n",
    "\n",
    "#Añadimos nuevos algoritmos de clasificación (no supervisado):\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib as mpl\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler  #Escalar\n",
    "from sklearn.preprocessing import StandardScaler #Estandarizar\n",
    "from sklearn.preprocessing import Normalizer #Normalizar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=50)\n",
    "#%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "import warnings\n",
    "import math\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "result=\"./results/results_2.csv\" #a CSV file is named in which the results are saved.\n",
    "csv_files=[\"all_data_preproc.csv\"]# CSV files names: #The names of the dataset files (csv_files).\n",
    "path=\"\"\n",
    "repetition=10\n",
    "\n",
    "ml_list={\n",
    "\"LOF\": LocalOutlierFactor(n_neighbors=25, contamination=0.2) #Sabemos que el 30% de los datos corresponden con ataques => contamination=0.3\n",
    "}\n",
    "\n",
    "# the features to be used for each attack type is defined in a dictionary(features).\n",
    "# the first 4 of the features created by the file \"04_1_feature_selection_for_attack_files.py\" are used here.\n",
    "features={  'all_data_preproc': [\"dsport\", \"sbytes\", \"sport\", \"Sload\", \"dbytes\", \"Spkts\", \"dstip\", \"Label\"]}\n",
    "\n",
    "seconds=time.time()#time stamp for all processing time\n",
    "\n",
    "\n",
    "print ('%-17s %-27s  %-15s %-15s %-15s %-15s %-15s' % (\"File\",\"ML algorithm\",\"Accuracy\",\"Precision\", \"Recall\" , \"F1-score\",\"Time\"))# print output header\n",
    "\n",
    "\n",
    "for j in csv_files: #this loop runs on the list containing the filenames.Operations are repeated for all attack files\n",
    "    a=[]\n",
    "    \n",
    "    feature_list=list(features[j[0:-4]])\n",
    "    df=pd.read_csv(path+j,usecols=feature_list)#read an attack file.\n",
    "    df=df.fillna(0)\n",
    "    attack_or_not=[]\n",
    "    for i in df[\"Label\"]: #it changes the normal label to \"1\" and the attack tag to \"0\" for use in the machine learning algorithm\n",
    "        \n",
    "        if i ==\"BENIGN\":\n",
    "            attack_or_not.append(1)\n",
    "        else:\n",
    "            attack_or_not.append(0)\n",
    "    df[\"Label\"]=attack_or_not\n",
    "\n",
    "    \n",
    "    y = df[\"Label\"] #this section separates the label and the data into two separate pieces, as Label=y Data=X \n",
    "    del df[\"Label\"]\n",
    "    feature_list.remove('Label')\n",
    "    X = df[feature_list]\n",
    "    \n",
    "\n",
    "    for ii in ml_list:\n",
    "        precision=[]\n",
    "        recall=[]\n",
    "        f1=[]\n",
    "        accuracy=[]\n",
    "        t_time=[]\n",
    "        \n",
    "        for i in range(repetition): # This loop allows cross-validation and machine learning algorithm to be repeated 10 times\n",
    "            #for i in range(repetition): # This loop allows cross-validation and machine learning algorithm to be repeated 10 times\n",
    "            second=time.time()#time stamp for processing time\n",
    "\n",
    "            # cross-validation\n",
    "            scaler = StandardScaler()\n",
    "            X = scaler.fit_transform(X)\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y,#  data (X) and labels (y) are divided into 2 parts to be sent to the machine learning algorithm (80% train,%20 test). \n",
    "                test_size = 0.20, random_state = repetition)#  So, in total there are 4 tracks: training data(X_train), training tag (y_train), test data(X_test) and test tag(y_test).\n",
    "\n",
    "            clf = ml_list[ii]#choose algorithm from ml_list dictionary                                                                          \n",
    "            clf.fit(X_train)\n",
    "            predict =clf.fit_predict(X_test)\n",
    "\n",
    "            predict = np.where(predict == -1, 0, predict) #El resultado de LOF son 1 para los normales y -1 para anomalías\n",
    "\n",
    "            #makes \"classification report\" and assigns the precision, f-measure, and recall values.s.    \n",
    "            f_1=f1_score(y_test, predict, average='macro')\n",
    "            pr=precision_score(y_test, predict, average='macro')\n",
    "            rc=recall_score(y_test, predict, average='macro')\n",
    "            score = metrics.accuracy_score(y_test, predict)\n",
    "\n",
    "            precision.append(float(pr))\n",
    "            recall.append(float(rc))\n",
    "            f1.append(float(f_1))\n",
    "            accuracy.append(score)\n",
    "            t_time.append(float((time.time()-second)))\n",
    "\n",
    "        print ('%-17s %-27s  %-15s %-15s %-15s %-15s %-15s' % (j[0:-4],ii,str(round(np.mean(accuracy),2)),str(round(np.mean(precision),2)), \n",
    "                str(round(np.mean(recall),2)),str(round(np.mean(f1),2)),str(round(np.mean(t_time),4))))#the result of the ten repetitions is printed on the screen\n",
    "\n",
    "\n",
    "    #print(\"\\n------------------------------------------------------------------------------------------------------\\n\\n\")\n",
    "    \n",
    "print(\"mission accomplished!\")\n",
    "print(\"Total operation time: = \",time.time()- seconds ,\"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File              ML algorithm                 Accuracy        Precision       Recall          F1-score        Time           \n",
      "all_data_preproc  K-Means                      0.85            0.44            0.48            0.46            8.4687         \n",
      "mission accomplished!\n",
      "Total operation time: =  93.44380450248718 seconds\n"
     ]
    }
   ],
   "source": [
    "## ALL_DATA: 12 LABELS - K-MEDIAS\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.cluster import completeness_score\n",
    "\n",
    "#Añadimos nuevos algoritmos de clasificación (no supervisado):\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib as mpl\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler  #Escalar\n",
    "from sklearn.preprocessing import StandardScaler #Estandarizar\n",
    "from sklearn.preprocessing import Normalizer #Normalizar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=50)\n",
    "#%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "import warnings\n",
    "import math\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "result=\"./results/results_2.csv\" #a CSV file is named in which the results are saved.\n",
    "csv_files=[\"all_data_preproc.csv\"]# CSV files names: #The names of the dataset files (csv_files).\n",
    "path=\"\"\n",
    "repetition=10\n",
    "\n",
    "# the features to be used for each attack type is defined in a dictionary(features).\n",
    "# the first 4 of the features created by the file \"04_1_feature_selection_for_attack_files.py\" are used here.\n",
    "features={ 'all_data_preproc': [\"service\",\"dbytes\",\"dsport\",\"Sload\",\"sttl\",\"sbytes\",\"sloss\",\"sport\",\"proto\",\"Dpkts\",\"dstip\",\"Dload\", \"Label\"]}\n",
    "\n",
    "seconds=time.time()#time stamp for all processing time\n",
    "\n",
    "\n",
    "print ('%-17s %-27s  %-15s %-15s %-15s %-15s %-15s' % (\"File\",\"ML algorithm\",\"Accuracy\",\"Precision\", \"Recall\" , \"F1-score\",\"Time\"))# print output header\n",
    "\n",
    "\n",
    "for j in csv_files: #this loop runs on the list containing the filenames.Operations are repeated for all attack files\n",
    "    a=[]\n",
    "    \n",
    "    feature_list=list(features[j[0:-4]])\n",
    "    df=pd.read_csv(path+j,usecols=feature_list)#read an attack file.\n",
    "    df=df.fillna(0)\n",
    "    attack_or_not=[]\n",
    "    for i in df[\"Label\"]: #it changes the normal label to \"1\" and the attack tag to \"0\" for use in the machine learning algorithm\n",
    "        \n",
    "        if i ==\"BENIGN\":\n",
    "            attack_or_not.append(1)\n",
    "        else:\n",
    "            attack_or_not.append(0)\n",
    "    df[\"Label\"]=attack_or_not\n",
    "\n",
    "    \n",
    "    y = df[\"Label\"] #this section separates the label and the data into two separate pieces, as Label=y Data=X \n",
    "    del df[\"Label\"]\n",
    "    feature_list.remove('Label')\n",
    "    X = df[feature_list]\n",
    "    \n",
    "    precision=[]\n",
    "    recall=[]\n",
    "    f1=[]\n",
    "    accuracy=[]\n",
    "    t_time=[]\n",
    "    \n",
    "    for i in range(repetition): # This loop allows cross-validation and machine learning algorithm to be repeated 10 times\n",
    "        #for i in range(repetition): # This loop allows cross-validation and machine learning algorithm to be repeated 10 times\n",
    "        second=time.time()#time stamp for processing time\n",
    "\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "\n",
    "        # cross-validation\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y,#  data (X) and labels (y) are divided into 2 parts to be sent to the machine learning algorithm (80% train,%20 test). \n",
    "            test_size = 0.20, random_state = repetition)#  So, in total there are 4 tracks: training data(X_train), training tag (y_train), test data(X_test) and test tag(y_test).\n",
    "\n",
    "        #print(\"  Numero de clusters: %d\\n\" % (2))\n",
    "        km = KMeans(init='random', n_clusters=2, random_state=333)\n",
    "        km.fit(X_train)     \n",
    "        predict = km.predict(X_test)\n",
    "\n",
    "        #Para poder identificar qué cluster corresponde\n",
    "        sum0 = sum(1 for p in predict if p == 0)\n",
    "        sum1 = sum(1 for p in predict if p == 1)\n",
    "        if(sum0 > sum1):\n",
    "            predict = predict + 1\n",
    "            predict = np.where(predict == 2, 0, predict)\n",
    "\n",
    "\n",
    "        f_1=f1_score(y_test, predict, average='macro')\n",
    "        pr=precision_score(y_test, predict, average='macro')\n",
    "        rc=recall_score(y_test, predict, average='macro')\n",
    "        score = metrics.accuracy_score(y_test, predict)\n",
    "\n",
    "        precision.append(float(pr))\n",
    "        recall.append(float(rc))\n",
    "        f1.append(float(f_1))\n",
    "        accuracy.append(score)\n",
    "        t_time.append(float((time.time()-second)))\n",
    "\n",
    "    print ('%-17s %-27s  %-15s %-15s %-15s %-15s %-15s' % (j[0:-4],\"K-Means\",str(round(np.mean(accuracy),2)),str(round(np.mean(precision),2)), \n",
    "            str(round(np.mean(recall),2)),str(round(np.mean(f1),2)),str(round(np.mean(t_time),4))))#the result of the ten repetitions is printed on the screen.\n",
    "\n",
    "\n",
    "\n",
    "    #print(\"\\n------------------------------------------------------------------------------------------------------\\n\\n\")\n",
    "    \n",
    "print(\"mission accomplished!\")\n",
    "print(\"Total operation time: = \",time.time()- seconds ,\"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File              ML algorithm                 Accuracy        Precision       Recall          F1-score        Time           \n",
      "all_data_preproc  K-Medoids                    0.61            0.6             0.72            0.54            5.2122         \n",
      "mission accomplished!\n",
      "Total operation time: =  60.09671664237976 seconds\n"
     ]
    }
   ],
   "source": [
    "## ALL_DATA: 12 LABELS - K-MEDOIDS\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.cluster import completeness_score\n",
    "\n",
    "#Añadimos nuevos algoritmos de clasificación (no supervisado):\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib as mpl\n",
    "import math\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib as mpl\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler  #Escalar\n",
    "from sklearn.preprocessing import StandardScaler #Estandarizar\n",
    "from sklearn.preprocessing import Normalizer #Normalizar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=50)\n",
    "#%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "import warnings\n",
    "import math\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "result=\"./results/results_2.csv\" #a CSV file is named in which the results are saved.\n",
    "csv_files=[\"all_data_preproc.csv\"]# CSV files names: #The names of the dataset files (csv_files).\n",
    "path=\"\"\n",
    "repetition=10\n",
    "\n",
    "# the features to be used for each attack type is defined in a dictionary(features).\n",
    "# the first 4 of the features created by the file \"04_1_feature_selection_for_attack_files.py\" are used here.\n",
    "features={ 'all_data_preproc': [\"service\",\"dbytes\",\"dsport\",\"Sload\",\"sttl\",\"sbytes\",\"sloss\",\"sport\",\"proto\",\"Dpkts\",\"dstip\",\"Dload\", \"Label\"]}\n",
    "\n",
    "seconds=time.time()#time stamp for all processing time\n",
    "\n",
    "\n",
    "print ('%-17s %-27s  %-15s %-15s %-15s %-15s %-15s' % (\"File\",\"ML algorithm\",\"Accuracy\",\"Precision\", \"Recall\" , \"F1-score\",\"Time\"))# print output header\n",
    "\n",
    "\n",
    "for j in csv_files: #this loop runs on the list containing the filenames.Operations are repeated for all attack files\n",
    "    a=[]\n",
    "    \n",
    "    feature_list=list(features[j[0:-4]])\n",
    "    df=pd.read_csv(path+j,usecols=feature_list)#read an attack file.\n",
    "    df=df.fillna(0)\n",
    "    numRows = min(df.shape[0]-1, 10000) #Para que no explote el consumo de memoria\n",
    "    df = df.sample(numRows)\n",
    "    attack_or_not=[]\n",
    "    for i in df[\"Label\"]: #it changes the normal label to \"1\" and the attack tag to \"0\" for use in the machine learning algorithm\n",
    "        \n",
    "        if i ==\"BENIGN\":\n",
    "            attack_or_not.append(1)\n",
    "        else:\n",
    "            attack_or_not.append(0)\n",
    "    df[\"Label\"]=attack_or_not\n",
    "\n",
    "    \n",
    "    y = df[\"Label\"] #this section separates the label and the data into two separate pieces, as Label=y Data=X \n",
    "    del df[\"Label\"]\n",
    "    feature_list.remove('Label')\n",
    "    X = df[feature_list]\n",
    "    \n",
    "    precision=[]\n",
    "    recall=[]\n",
    "    f1=[]\n",
    "    accuracy=[]\n",
    "    t_time=[]\n",
    "    \n",
    "    for i in range(repetition): # This loop allows cross-validation and machine learning algorithm to be repeated 10 times\n",
    "        #for i in range(repetition): # This loop allows cross-validation and machine learning algorithm to be repeated 10 times\n",
    "        second=time.time()#time stamp for processing time\n",
    "\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "        \n",
    "        # cross-validation\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y,#  data (X) and labels (y) are divided into 2 parts to be sent to the machine learning algorithm (80% train,%20 test). \n",
    "            test_size = 0.20, random_state = repetition)#  So, in total there are 4 tracks: training data(X_train), training tag (y_train), test data(X_test) and test tag(y_test).\n",
    "\n",
    "        #print(\"  Numero de clusters: %d\\n\" % (2))\n",
    "        km = KMedoids(init='random', n_clusters=2, random_state=333)\n",
    "        km.fit(X_train)     \n",
    "        predict = km.predict(X_test)\n",
    "\n",
    "        #Para poder identificar qué cluster corresponde\n",
    "        sum0 = sum(1 for p in predict if p == 0)\n",
    "        sum1 = sum(1 for p in predict if p == 1)\n",
    "        if(sum0 > sum1):\n",
    "            predict = predict + 1\n",
    "            predict = np.where(predict == 2, 0, predict)\n",
    "\n",
    "\n",
    "        f_1=f1_score(y_test, predict, average='macro')\n",
    "        pr=precision_score(y_test, predict, average='macro')\n",
    "        rc=recall_score(y_test, predict, average='macro')\n",
    "        score = metrics.accuracy_score(y_test, predict)\n",
    "\n",
    "        precision.append(float(pr))\n",
    "        recall.append(float(rc))\n",
    "        f1.append(float(f_1))\n",
    "        accuracy.append(score)\n",
    "        t_time.append(float((time.time()-second)))\n",
    "\n",
    "    print ('%-17s %-27s  %-15s %-15s %-15s %-15s %-15s' % (j[0:-4],\"K-Medoids\",str(round(np.mean(accuracy),2)),str(round(np.mean(precision),2)), \n",
    "            str(round(np.mean(recall),2)),str(round(np.mean(f1),2)),str(round(np.mean(t_time),4))))#the result of the ten repetitions is printed on the screen.\n",
    "\n",
    "    #print(\"\\n------------------------------------------------------------------------------------------------------\\n\\n\")\n",
    "    \n",
    "print(\"mission accomplished!\")\n",
    "print(\"Total operation time: = \",time.time()- seconds ,\"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File              ML algorithm                 Accuracy        Precision       Recall          F1-score        Time           \n",
      "all_data_preproc  Isolation Forest             0.68            0.54            0.57            0.53            8.452          \n",
      "mission accomplished!\n",
      "Total operation time: =  94.55759477615356 seconds\n"
     ]
    }
   ],
   "source": [
    "## ALL_DATA: 12 LABELS - Isolation Forest\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.cluster import completeness_score\n",
    "\n",
    "#Añadimos nuevos algoritmos de clasificación (no supervisado):\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib as mpl\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler  #Escalar\n",
    "from sklearn.preprocessing import StandardScaler #Estandarizar\n",
    "from sklearn.preprocessing import Normalizer #Normalizar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=50)\n",
    "#%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "import warnings\n",
    "import math\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "result=\"./results/results_2.csv\" #a CSV file is named in which the results are saved.\n",
    "csv_files=[\"all_data_preproc.csv\"]# CSV files names: #The names of the dataset files (csv_files).\n",
    "path=\"\"\n",
    "repetition=10\n",
    "\n",
    "ml_list={\n",
    "\"Isolation Forest\": IsolationForest(random_state=123, n_estimators = 30, contamination = 0.3, max_samples = \"auto\") #Sabemos que el 30% de los datos corresponden con ataques => contamination=0.3\n",
    "}\n",
    "\n",
    "# the features to be used for each attack type is defined in a dictionary(features).\n",
    "# the first 4 of the features created by the file \"04_1_feature_selection_for_attack_files.py\" are used here.\n",
    "features={ 'all_data_preproc': [\"service\",\"dbytes\",\"dsport\",\"Sload\",\"sttl\",\"sbytes\",\"sloss\",\"sport\",\"proto\",\"Dpkts\",\"dstip\",\"Dload\", \"Label\"]}\n",
    "\n",
    "seconds=time.time()#time stamp for all processing time\n",
    "\n",
    "\n",
    "print ('%-17s %-27s  %-15s %-15s %-15s %-15s %-15s' % (\"File\",\"ML algorithm\",\"Accuracy\",\"Precision\", \"Recall\" , \"F1-score\",\"Time\"))# print output header\n",
    "\n",
    "\n",
    "for j in csv_files: #this loop runs on the list containing the filenames.Operations are repeated for all attack files\n",
    "    a=[]\n",
    "    \n",
    "    feature_list=list(features[j[0:-4]])\n",
    "    df=pd.read_csv(path+j,usecols=feature_list)#read an attack file.\n",
    "    df=df.fillna(0)\n",
    "    attack_or_not=[]\n",
    "    for i in df[\"Label\"]: #it changes the normal label to \"1\" and the attack tag to \"0\" for use in the machine learning algorithm\n",
    "        \n",
    "        if i ==\"BENIGN\":\n",
    "            attack_or_not.append(1)\n",
    "        else:\n",
    "            attack_or_not.append(0)\n",
    "    df[\"Label\"]=attack_or_not\n",
    "\n",
    "    \n",
    "    y = df[\"Label\"] #this section separates the label and the data into two separate pieces, as Label=y Data=X \n",
    "    del df[\"Label\"]\n",
    "    feature_list.remove('Label')\n",
    "    X = df[feature_list]\n",
    "    \n",
    "    for ii in ml_list:\n",
    "        precision=[]\n",
    "        recall=[]\n",
    "        f1=[]\n",
    "        accuracy=[]\n",
    "        t_time=[]\n",
    "        \n",
    "        for i in range(repetition): # This loop allows cross-validation and machine learning algorithm to be repeated 10 times\n",
    "            #for i in range(repetition): # This loop allows cross-validation and machine learning algorithm to be repeated 10 times\n",
    "            second=time.time()#time stamp for processing time\n",
    "\n",
    "            # cross-validation\n",
    "            scaler = StandardScaler()\n",
    "            X = scaler.fit_transform(X)\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y,#  data (X) and labels (y) are divided into 2 parts to be sent to the machine learning algorithm (80% train,%20 test). \n",
    "                test_size = 0.20, random_state = repetition)#  So, in total there are 4 tracks: training data(X_train), training tag (y_train), test data(X_test) and test tag(y_test).\n",
    "\n",
    "            #scaler = MinMaxScaler()\n",
    "            #scaler = StandardScaler()\n",
    "            #scaler = Normalizer()\n",
    "            #X_train = scaler.fit_transform(X_train)\n",
    "            #X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "            clf = ml_list[ii]#choose algorithm from ml_list dictionary                                                                          \n",
    "            clf.fit(X_train)\n",
    "            predict =clf.predict(X_test)\n",
    "\n",
    "            predict = np.where(predict == -1, 0, predict) #El resultado de isolation forest son 1 para los normales y -1 para anomalías\n",
    "\n",
    "            #makes \"classification report\" and assigns the precision, f-measure, and recall values.s.    \n",
    "            f_1=f1_score(y_test, predict, average='macro')\n",
    "            pr=precision_score(y_test, predict, average='macro')\n",
    "            rc=recall_score(y_test, predict, average='macro')\n",
    "            score = metrics.accuracy_score(y_test, predict)\n",
    "\n",
    "            precision.append(float(pr))\n",
    "            recall.append(float(rc))\n",
    "            f1.append(float(f_1))\n",
    "            accuracy.append(score)\n",
    "            t_time.append(float((time.time()-second)))\n",
    "\n",
    "        print ('%-17s %-27s  %-15s %-15s %-15s %-15s %-15s' % (j[0:-4],ii,str(round(np.mean(accuracy),2)),str(round(np.mean(precision),2)), \n",
    "                str(round(np.mean(recall),2)),str(round(np.mean(f1),2)),str(round(np.mean(t_time),4))))#the result of the ten repetitions is printed on the screen.\n",
    "\n",
    "\n",
    "\n",
    "    #print(\"\\n------------------------------------------------------------------------------------------------------\\n\\n\")\n",
    "    \n",
    "print(\"mission accomplished!\")\n",
    "print(\"Total operation time: = \",time.time()- seconds ,\"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ALL_DATA: 12 LABELS - Local Outlier Factor\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.cluster import completeness_score\n",
    "\n",
    "#Añadimos nuevos algoritmos de clasificación (no supervisado):\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib as mpl\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler  #Escalar\n",
    "from sklearn.preprocessing import StandardScaler #Estandarizar\n",
    "from sklearn.preprocessing import Normalizer #Normalizar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=50)\n",
    "#%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "import warnings\n",
    "import math\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "result=\"./results/results_2.csv\" #a CSV file is named in which the results are saved.\n",
    "csv_files=[\"all_data_preproc.csv\"]# CSV files names: #The names of the dataset files (csv_files).\n",
    "path=\"\"\n",
    "repetition=10\n",
    "\n",
    "ml_list={\n",
    "\"LOF\": LocalOutlierFactor(n_neighbors=25, contamination=0.2) #Sabemos que el 30% de los datos corresponden con ataques => contamination=0.3\n",
    "}\n",
    "\n",
    "# the features to be used for each attack type is defined in a dictionary(features).\n",
    "# the first 4 of the features created by the file \"04_1_feature_selection_for_attack_files.py\" are used here.\n",
    "features={ 'all_data_preproc': [\"service\",\"dbytes\",\"dsport\",\"Sload\",\"sttl\",\"sbytes\",\"sloss\",\"sport\",\"proto\",\"Dpkts\",\"dstip\",\"Dload\", \"Label\"]}\n",
    "\n",
    "seconds=time.time()#time stamp for all processing time\n",
    "\n",
    "\n",
    "print ('%-17s %-27s  %-15s %-15s %-15s %-15s %-15s' % (\"File\",\"ML algorithm\",\"Accuracy\",\"Precision\", \"Recall\" , \"F1-score\",\"Time\"))# print output header\n",
    "\n",
    "\n",
    "for j in csv_files: #this loop runs on the list containing the filenames.Operations are repeated for all attack files\n",
    "    a=[]\n",
    "    \n",
    "    feature_list=list(features[j[0:-4]])\n",
    "    df=pd.read_csv(path+j,usecols=feature_list)#read an attack file.\n",
    "    df=df.fillna(0)\n",
    "    attack_or_not=[]\n",
    "    for i in df[\"Label\"]: #it changes the normal label to \"1\" and the attack tag to \"0\" for use in the machine learning algorithm\n",
    "        \n",
    "        if i ==\"BENIGN\":\n",
    "            attack_or_not.append(1)\n",
    "        else:\n",
    "            attack_or_not.append(0)\n",
    "    df[\"Label\"]=attack_or_not\n",
    "\n",
    "    \n",
    "    y = df[\"Label\"] #this section separates the label and the data into two separate pieces, as Label=y Data=X \n",
    "    del df[\"Label\"]\n",
    "    feature_list.remove('Label')\n",
    "    X = df[feature_list]\n",
    "    \n",
    "\n",
    "    for ii in ml_list:\n",
    "        precision=[]\n",
    "        recall=[]\n",
    "        f1=[]\n",
    "        accuracy=[]\n",
    "        t_time=[]\n",
    "        \n",
    "        for i in range(repetition): # This loop allows cross-validation and machine learning algorithm to be repeated 10 times\n",
    "            #for i in range(repetition): # This loop allows cross-validation and machine learning algorithm to be repeated 10 times\n",
    "            second=time.time()#time stamp for processing time\n",
    "\n",
    "            # cross-validation\n",
    "            scaler = StandardScaler()\n",
    "            X = scaler.fit_transform(X)\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y,#  data (X) and labels (y) are divided into 2 parts to be sent to the machine learning algorithm (80% train,%20 test). \n",
    "                test_size = 0.20, random_state = repetition)#  So, in total there are 4 tracks: training data(X_train), training tag (y_train), test data(X_test) and test tag(y_test).\n",
    "\n",
    "            clf = ml_list[ii]#choose algorithm from ml_list dictionary                                                                          \n",
    "            clf.fit(X_train)\n",
    "            predict =clf.fit_predict(X_test)\n",
    "\n",
    "            predict = np.where(predict == -1, 0, predict) #El resultado de LOF son 1 para los normales y -1 para anomalías\n",
    "\n",
    "            #makes \"classification report\" and assigns the precision, f-measure, and recall values.s.    \n",
    "            f_1=f1_score(y_test, predict, average='macro')\n",
    "            pr=precision_score(y_test, predict, average='macro')\n",
    "            rc=recall_score(y_test, predict, average='macro')\n",
    "            score = metrics.accuracy_score(y_test, predict)\n",
    "\n",
    "            precision.append(float(pr))\n",
    "            recall.append(float(rc))\n",
    "            f1.append(float(f_1))\n",
    "            accuracy.append(score)\n",
    "            t_time.append(float((time.time()-second)))\n",
    "\n",
    "        print ('%-17s %-27s  %-15s %-15s %-15s %-15s %-15s' % (j[0:-4],ii,str(round(np.mean(accuracy),2)),str(round(np.mean(precision),2)), \n",
    "                str(round(np.mean(recall),2)),str(round(np.mean(f1),2)),str(round(np.mean(t_time),4))))#the result of the ten repetitions is printed on the screen\n",
    "\n",
    "\n",
    "    #print(\"\\n------------------------------------------------------------------------------------------------------\\n\\n\")\n",
    "    \n",
    "print(\"mission accomplished!\")\n",
    "print(\"Total operation time: = \",time.time()- seconds ,\"seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
