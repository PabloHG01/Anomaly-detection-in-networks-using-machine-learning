{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CICIDS 2017\n",
    "Implementación de los IDS para el conjunto total de datos usando algoritmos de aprendizaje no supervisado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ALL_DATA: 7 LABELS - K-MEDIAS\n",
    "\n",
    "# Implementación del IDS usando los 7 parámetros con mayor importancia para el dataset completo.\n",
    "# Algoritmo K-Medias\n",
    "\n",
    "# Para la ejecución de este script se necesita el archivo \"all_data.csv\".\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.cluster import completeness_score\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib as mpl\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler  #Escalar\n",
    "from sklearn.preprocessing import StandardScaler #Estandarizar\n",
    "from sklearn.preprocessing import Normalizer #Normalizar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=50)\n",
    "#%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "import warnings\n",
    "import math\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "csv_files=[\"all_data.csv\"]# CSV files names: #The names of the dataset files (csv_files).\n",
    "path=\"\"\n",
    "\n",
    "repetition=10\n",
    "\n",
    "# Los siguientes 7 parámetros son aquellos con mayor importancia para el dataset completo:\n",
    "\n",
    "features={\"all_data\":[\"Bwd Packet Length Std\", \"Flow Bytes/s\", \"Total Length of Fwd Packets\", \"Fwd Packet Length Std\",\n",
    "     \"Flow IAT Std\", \"Flow IAT Min\", \"Fwd IAT Total\",\"Label\"]}\n",
    "\n",
    "seconds=time.time()#time stamp for all processing time\n",
    "\n",
    "\n",
    "print ('%-17s %-27s  %-15s %-15s %-15s %-15s %-15s' % (\"File\",\"ML algorithm\",\"Accuracy\",\"Precision\", \"Recall\" , \"F1-score\",\"Time\"))# print output header\n",
    "\n",
    "\n",
    "for j in csv_files: #this loop runs on the list containing the filenames.Operations are repeated for all attack files\n",
    "    a=[]\n",
    "    \n",
    "    feature_list=list(features[j[0:-4]])\n",
    "    df=pd.read_csv(path+j,usecols=feature_list)#read an attack file.\n",
    "    df=df.fillna(0)\n",
    "    attack_or_not=[]\n",
    "    for i in df[\"Label\"]: #it changes the normal label to \"1\" and the attack tag to \"0\" for use in the machine learning algorithm\n",
    "        \n",
    "        if i ==\"BENIGN\":\n",
    "            attack_or_not.append(1)\n",
    "        else:\n",
    "            attack_or_not.append(0)\n",
    "    df[\"Label\"]=attack_or_not\n",
    "\n",
    "    \n",
    "    y = df[\"Label\"] #this section separates the label and the data into two separate pieces, as Label=y Data=X \n",
    "    del df[\"Label\"]\n",
    "    feature_list.remove('Label')\n",
    "    X = df[feature_list]\n",
    "    \n",
    "    precision=[]\n",
    "    recall=[]\n",
    "    f1=[]\n",
    "    accuracy=[]\n",
    "    t_time=[]\n",
    "    \n",
    "    for i in range(repetition): # This loop allows cross-validation and machine learning algorithm to be repeated 10 times\n",
    "        #for i in range(repetition): # This loop allows cross-validation and machine learning algorithm to be repeated 10 times\n",
    "        second=time.time()#time stamp for processing time\n",
    "\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "\n",
    "        # cross-validation\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y,#  data (X) and labels (y) are divided into 2 parts to be sent to the machine learning algorithm (80% train,%20 test). \n",
    "            test_size = 0.20, random_state = repetition)#  So, in total there are 4 tracks: training data(X_train), training tag (y_train), test data(X_test) and test tag(y_test).\n",
    "\n",
    "        #print(\"  Numero de clusters: %d\\n\" % (2))\n",
    "        km = KMeans(init='random', n_clusters=2, random_state=333)\n",
    "        km.fit(X_train)     \n",
    "        predict = km.predict(X_test)\n",
    "\n",
    "        #Para poder identificar qué cluster corresponde\n",
    "        sum0 = sum(1 for p in predict if p == 0)\n",
    "        sum1 = sum(1 for p in predict if p == 1)\n",
    "        if(sum0 > sum1):\n",
    "            predict = predict + 1\n",
    "            predict = np.where(predict == 2, 0, predict)\n",
    "\n",
    "\n",
    "        f_1=f1_score(y_test, predict, average='macro')\n",
    "        pr=precision_score(y_test, predict, average='macro')\n",
    "        rc=recall_score(y_test, predict, average='macro')\n",
    "        score = metrics.accuracy_score(y_test, predict)\n",
    "\n",
    "        precision.append(float(pr))\n",
    "        recall.append(float(rc))\n",
    "        f1.append(float(f_1))\n",
    "        accuracy.append(score)\n",
    "        t_time.append(float((time.time()-second)))\n",
    "\n",
    "    print ('%-17s %-27s  %-15s %-15s %-15s %-15s %-15s' % (j[0:-4],\"K-Means\",str(round(np.mean(accuracy),2)),str(round(np.mean(precision),2)), \n",
    "            str(round(np.mean(recall),2)),str(round(np.mean(f1),2)),str(round(np.mean(t_time),4))))#the result of the ten repetitions is printed on the screen.\n",
    "\n",
    "    \n",
    "print(\"Procesamiento completado\")\n",
    "print(\"Tiempo total de cómputo: = \",time.time()- seconds ,\"segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ALL_DATA: 7 LABELS - K-MEDOIDS\n",
    "\n",
    "# Implementación del IDS usando los 7 parámetros con mayor importancia para el dataset completo.\n",
    "# Algoritmo K-Medoids\n",
    "\n",
    "# Para la ejecución de este script se necesita el archivo \"all_data.csv\".\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.cluster import completeness_score\n",
    "\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib as mpl\n",
    "import math\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib as mpl\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler  #Escalar\n",
    "from sklearn.preprocessing import StandardScaler #Estandarizar\n",
    "from sklearn.preprocessing import Normalizer #Normalizar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=50)\n",
    "#%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "import warnings\n",
    "import math\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "csv_files=[\"all_data.csv\"]# CSV files names: #The names of the dataset files (csv_files).\n",
    "path=\"\"\n",
    "\n",
    "repetition=10\n",
    "\n",
    "# Los siguientes 7 parámetros son aquellos con mayor importancia para el dataset completo:\n",
    "\n",
    "features={\"all_data\":[\"Bwd Packet Length Std\", \"Flow Bytes/s\", \"Total Length of Fwd Packets\", \"Fwd Packet Length Std\",\n",
    "     \"Flow IAT Std\", \"Flow IAT Min\", \"Fwd IAT Total\",\"Label\"]}\n",
    "\n",
    "seconds=time.time()#time stamp for all processing time\n",
    "\n",
    "\n",
    "print ('%-17s %-27s  %-15s %-15s %-15s %-15s %-15s' % (\"File\",\"ML algorithm\",\"Accuracy\",\"Precision\", \"Recall\" , \"F1-score\",\"Time\"))# print output header\n",
    "\n",
    "\n",
    "for j in csv_files: #this loop runs on the list containing the filenames.Operations are repeated for all attack files\n",
    "    a=[]\n",
    "    \n",
    "    feature_list=list(features[j[0:-4]])\n",
    "    df=pd.read_csv(path+j,usecols=feature_list)#read an attack file.\n",
    "    df=df.fillna(0)\n",
    "\n",
    "    attack_or_not=[]\n",
    "    for i in df[\"Label\"]: #it changes the normal label to \"1\" and the attack tag to \"0\" for use in the machine learning algorithm\n",
    "        \n",
    "        if i ==\"BENIGN\":\n",
    "            attack_or_not.append(1)\n",
    "        else:\n",
    "            attack_or_not.append(0)\n",
    "    df[\"Label\"]=attack_or_not\n",
    "\n",
    "    \n",
    "    y = df[\"Label\"] #this section separates the label and the data into two separate pieces, as Label=y Data=X \n",
    "    del df[\"Label\"]\n",
    "    feature_list.remove('Label')\n",
    "    X = df[feature_list]\n",
    "    \n",
    "    precision=[]\n",
    "    recall=[]\n",
    "    f1=[]\n",
    "    accuracy=[]\n",
    "    t_time=[]\n",
    "    \n",
    "    for i in range(repetition): # This loop allows cross-validation and machine learning algorithm to be repeated 10 times\n",
    "        #for i in range(repetition): # This loop allows cross-validation and machine learning algorithm to be repeated 10 times\n",
    "        second=time.time()#time stamp for processing time\n",
    "\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "        \n",
    "        # cross-validation\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y,#  data (X) and labels (y) are divided into 2 parts to be sent to the machine learning algorithm (80% train,%20 test). \n",
    "            test_size = 0.20, random_state = repetition)#  So, in total there are 4 tracks: training data(X_train), training tag (y_train), test data(X_test) and test tag(y_test).\n",
    "\n",
    "        #print(\"  Numero de clusters: %d\\n\" % (2))\n",
    "        km = KMedoids(init='random', n_clusters=2, random_state=333)\n",
    "        km.fit(X_train)     \n",
    "        predict = km.predict(X_test)\n",
    "\n",
    "        #Para poder identificar qué cluster corresponde\n",
    "        sum0 = sum(1 for p in predict if p == 0)\n",
    "        sum1 = sum(1 for p in predict if p == 1)\n",
    "        if(sum0 > sum1):\n",
    "            predict = predict + 1\n",
    "            predict = np.where(predict == 2, 0, predict)\n",
    "\n",
    "\n",
    "        f_1=f1_score(y_test, predict, average='macro')\n",
    "        pr=precision_score(y_test, predict, average='macro')\n",
    "        rc=recall_score(y_test, predict, average='macro')\n",
    "        score = metrics.accuracy_score(y_test, predict)\n",
    "\n",
    "        precision.append(float(pr))\n",
    "        recall.append(float(rc))\n",
    "        f1.append(float(f_1))\n",
    "        accuracy.append(score)\n",
    "        t_time.append(float((time.time()-second)))\n",
    "\n",
    "    print ('%-17s %-27s  %-15s %-15s %-15s %-15s %-15s' % (j[0:-4],\"K-Medoids\",str(round(np.mean(accuracy),2)),str(round(np.mean(precision),2)), \n",
    "            str(round(np.mean(recall),2)),str(round(np.mean(f1),2)),str(round(np.mean(t_time),4))))#the result of the ten repetitions is printed on the screen.\n",
    "\n",
    "\n",
    "    \n",
    "print(\"Procesamiento completado\")\n",
    "print(\"Tiempo total de cómputo: = \",time.time()- seconds ,\"segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ALL_DATA: 7 LABELS - Isolation Forest\n",
    "\n",
    "# Implementación del IDS usando los 7 parámetros con mayor importancia para el dataset completo.\n",
    "# Algoritmo Isolation Forest\n",
    "\n",
    "# Para la ejecución de este script se necesita el archivo \"all_data.csv\".\n",
    "\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.cluster import completeness_score\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib as mpl\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler  #Escalar\n",
    "from sklearn.preprocessing import StandardScaler #Estandarizar\n",
    "from sklearn.preprocessing import Normalizer #Normalizar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=50)\n",
    "#%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "import warnings\n",
    "import math\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "csv_files=[\"all_data.csv\"]# CSV files names: #The names of the dataset files (csv_files).\n",
    "path=\"\"\n",
    "\n",
    "repetition=10\n",
    "\n",
    "ml_list={\n",
    "\"Isolation Forest\": IsolationForest(random_state=123, n_estimators = 30, contamination = 0.3, max_samples = \"auto\") #Sabemos que el 30% de los datos corresponden con ataques => contamination=0.3\n",
    "}\n",
    "\n",
    "# Los siguientes 7 parámetros son aquellos con mayor importancia para el dataset completo:\n",
    "\n",
    "features={\"all_data\":[\"Bwd Packet Length Std\", \"Flow Bytes/s\", \"Total Length of Fwd Packets\", \"Fwd Packet Length Std\",\n",
    "     \"Flow IAT Std\", \"Flow IAT Min\", \"Fwd IAT Total\",\"Label\"]}\n",
    "\n",
    "seconds=time.time()#time stamp for all processing time\n",
    "\n",
    "\n",
    "print ('%-17s %-27s  %-15s %-15s %-15s %-15s %-15s' % (\"File\",\"ML algorithm\",\"Accuracy\",\"Precision\", \"Recall\" , \"F1-score\",\"Time\"))# print output header\n",
    "\n",
    "\n",
    "for j in csv_files: #this loop runs on the list containing the filenames.Operations are repeated for all attack files\n",
    "    a=[]\n",
    "    \n",
    "    feature_list=list(features[j[0:-4]])\n",
    "    df=pd.read_csv(path+j,usecols=feature_list)#read an attack file.\n",
    "    df=df.fillna(0)\n",
    "    \n",
    "    attack_or_not=[]\n",
    "    for i in df[\"Label\"]: #it changes the normal label to \"1\" and the attack tag to \"0\" for use in the machine learning algorithm\n",
    "        \n",
    "        if i ==\"BENIGN\":\n",
    "            attack_or_not.append(1)\n",
    "        else:\n",
    "            attack_or_not.append(0)\n",
    "    df[\"Label\"]=attack_or_not\n",
    "\n",
    "    \n",
    "    y = df[\"Label\"] #this section separates the label and the data into two separate pieces, as Label=y Data=X \n",
    "    del df[\"Label\"]\n",
    "    feature_list.remove('Label')\n",
    "    X = df[feature_list]\n",
    "    \n",
    "    for ii in ml_list:\n",
    "        precision=[]\n",
    "        recall=[]\n",
    "        f1=[]\n",
    "        accuracy=[]\n",
    "        t_time=[]\n",
    "        \n",
    "        for i in range(repetition): # This loop allows cross-validation and machine learning algorithm to be repeated 10 times\n",
    "            #for i in range(repetition): # This loop allows cross-validation and machine learning algorithm to be repeated 10 times\n",
    "            second=time.time()#time stamp for processing time\n",
    "\n",
    "            # cross-validation\n",
    "            scaler = StandardScaler()\n",
    "            X = scaler.fit_transform(X)\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y,#  data (X) and labels (y) are divided into 2 parts to be sent to the machine learning algorithm (80% train,%20 test). \n",
    "                test_size = 0.20, random_state = repetition)#  So, in total there are 4 tracks: training data(X_train), training tag (y_train), test data(X_test) and test tag(y_test).\n",
    "\n",
    "\n",
    "            clf = ml_list[ii]#choose algorithm from ml_list dictionary                                                                          \n",
    "            clf.fit(X_train)\n",
    "            predict =clf.predict(X_test)\n",
    "\n",
    "            predict = np.where(predict == -1, 0, predict) #El resultado de isolation forest son 1 para los normales y -1 para anomalías\n",
    "\n",
    "            #makes \"classification report\" and assigns the precision, f-measure, and recall values.s.    \n",
    "            f_1=f1_score(y_test, predict, average='macro')\n",
    "            pr=precision_score(y_test, predict, average='macro')\n",
    "            rc=recall_score(y_test, predict, average='macro')\n",
    "            score = metrics.accuracy_score(y_test, predict)\n",
    "\n",
    "            precision.append(float(pr))\n",
    "            recall.append(float(rc))\n",
    "            f1.append(float(f_1))\n",
    "            accuracy.append(score)\n",
    "            t_time.append(float((time.time()-second)))\n",
    "\n",
    "        print ('%-17s %-27s  %-15s %-15s %-15s %-15s %-15s' % (j[0:-4],ii,str(round(np.mean(accuracy),2)),str(round(np.mean(precision),2)), \n",
    "                str(round(np.mean(recall),2)),str(round(np.mean(f1),2)),str(round(np.mean(t_time),4))))#the result of the ten repetitions is printed on the screen.\n",
    "\n",
    "\n",
    "    \n",
    "print(\"Procesamiento completado\")\n",
    "print(\"Tiempo total de cómputo: = \",time.time()- seconds ,\"segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ALL_DATA: 7 LABELS - Local Outlier Factor\n",
    "\n",
    "# Implementación del IDS usando los 7 parámetros con mayor importancia para el dataset completo.\n",
    "# Algoritmo Local Outlier Factor\n",
    "\n",
    "# Para la ejecución de este script se necesita el archivo \"all_data.csv\".\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.cluster import completeness_score\n",
    "\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib as mpl\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler  #Escalar\n",
    "from sklearn.preprocessing import StandardScaler #Estandarizar\n",
    "from sklearn.preprocessing import Normalizer #Normalizar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=50)\n",
    "#%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "import warnings\n",
    "import math\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "csv_files=[\"all_data.csv\"]# CSV files names: #The names of the dataset files (csv_files).\n",
    "path=\"\"\n",
    "\n",
    "repetition=10\n",
    "\n",
    "ml_list={\n",
    "\"LOF\": LocalOutlierFactor(n_neighbors=25, contamination=0.2) #Sabemos que el 30% de los datos corresponden con ataques => contamination=0.3\n",
    "}\n",
    "\n",
    "# Los siguientes 7 parámetros son aquellos con mayor importancia para el dataset completo:\n",
    "\n",
    "features={\"all_data\":[\"Bwd Packet Length Std\", \"Flow Bytes/s\", \"Total Length of Fwd Packets\", \"Fwd Packet Length Std\",\n",
    "     \"Flow IAT Std\", \"Flow IAT Min\", \"Fwd IAT Total\",\"Label\"]}\n",
    "\n",
    "seconds=time.time()#time stamp for all processing time\n",
    "\n",
    "\n",
    "print ('%-17s %-27s  %-15s %-15s %-15s %-15s %-15s' % (\"File\",\"ML algorithm\",\"Accuracy\",\"Precision\", \"Recall\" , \"F1-score\",\"Time\"))# print output header\n",
    "\n",
    "\n",
    "for j in csv_files: #this loop runs on the list containing the filenames.Operations are repeated for all attack files\n",
    "    a=[]\n",
    "    \n",
    "    feature_list=list(features[j[0:-4]])\n",
    "    df=pd.read_csv(path+j,usecols=feature_list)#read an attack file.\n",
    "    df=df.fillna(0)\n",
    "    \n",
    "    attack_or_not=[]\n",
    "    for i in df[\"Label\"]: #it changes the normal label to \"1\" and the attack tag to \"0\" for use in the machine learning algorithm\n",
    "        \n",
    "        if i ==\"BENIGN\":\n",
    "            attack_or_not.append(1)\n",
    "        else:\n",
    "            attack_or_not.append(0)\n",
    "    df[\"Label\"]=attack_or_not\n",
    "\n",
    "    \n",
    "    y = df[\"Label\"] #this section separates the label and the data into two separate pieces, as Label=y Data=X \n",
    "    del df[\"Label\"]\n",
    "    feature_list.remove('Label')\n",
    "    X = df[feature_list]\n",
    "    \n",
    "\n",
    "    for ii in ml_list:\n",
    "        precision=[]\n",
    "        recall=[]\n",
    "        f1=[]\n",
    "        accuracy=[]\n",
    "        t_time=[]\n",
    "        \n",
    "        for i in range(repetition): # This loop allows cross-validation and machine learning algorithm to be repeated 10 times\n",
    "            #for i in range(repetition): # This loop allows cross-validation and machine learning algorithm to be repeated 10 times\n",
    "            second=time.time()#time stamp for processing time\n",
    "\n",
    "            # cross-validation\n",
    "            scaler = StandardScaler()\n",
    "            X = scaler.fit_transform(X)\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y,#  data (X) and labels (y) are divided into 2 parts to be sent to the machine learning algorithm (80% train,%20 test). \n",
    "                test_size = 0.20, random_state = repetition)#  So, in total there are 4 tracks: training data(X_train), training tag (y_train), test data(X_test) and test tag(y_test).\n",
    "\n",
    "            clf = ml_list[ii]#choose algorithm from ml_list dictionary                                                                          \n",
    "            clf.fit(X_train)\n",
    "            predict =clf.fit_predict(X_test)\n",
    "\n",
    "            predict = np.where(predict == -1, 0, predict) #El resultado de LOF son 1 para los normales y -1 para anomalías\n",
    "\n",
    "            #makes \"classification report\" and assigns the precision, f-measure, and recall values.s.    \n",
    "            f_1=f1_score(y_test, predict, average='macro')\n",
    "            pr=precision_score(y_test, predict, average='macro')\n",
    "            rc=recall_score(y_test, predict, average='macro')\n",
    "            score = metrics.accuracy_score(y_test, predict)\n",
    "\n",
    "            precision.append(float(pr))\n",
    "            recall.append(float(rc))\n",
    "            f1.append(float(f_1))\n",
    "            accuracy.append(score)\n",
    "            t_time.append(float((time.time()-second)))\n",
    "\n",
    "        print ('%-17s %-27s  %-15s %-15s %-15s %-15s %-15s' % (j[0:-4],ii,str(round(np.mean(accuracy),2)),str(round(np.mean(precision),2)), \n",
    "                str(round(np.mean(recall),2)),str(round(np.mean(f1),2)),str(round(np.mean(t_time),4))))#the result of the ten repetitions is printed on the screen\n",
    "\n",
    "\n",
    "    \n",
    "print(\"Procesamiento completado\")\n",
    "print(\"Tiempo total de cómputo: = \",time.time()- seconds ,\"segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ALL_DATA: 18 LABELS - K-MEDIAS\n",
    "\n",
    "# Implementación de los IDS usando 18 parámetros procedentes de cada tipo de ataque\n",
    "# Algoritmo K-Medias\n",
    "\n",
    "# Para la ejecución de este script se necesita el archivo \"all_data.csv\".\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.cluster import completeness_score\n",
    "\n",
    "#Añadimos nuevos algoritmos de clasificación (no supervisado):\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib as mpl\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler  #Escalar\n",
    "from sklearn.preprocessing import StandardScaler #Estandarizar\n",
    "from sklearn.preprocessing import Normalizer #Normalizar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=50)\n",
    "#%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "import warnings\n",
    "import math\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "csv_files=[\"all_data.csv\"]# CSV files names: #The names of the dataset files (csv_files).\n",
    "path=\"\"\n",
    "\n",
    "repetition=10\n",
    "\n",
    "# Los siguientes 18 parámetros son resultado de combinar los 4 parámetros de mayor importancia para cada tipo de ataque\n",
    "\n",
    "features={\"all_data\":[\"Bwd Packet Length Max\",\"Bwd Packet Length Mean\",\"Bwd Packet Length Std\",\"Flow Bytes/s\",\n",
    "\"Flow Duration\",\"Flow IAT Max\",\"Flow IAT Mean\",\"Flow IAT Min\",\"Flow IAT Std\",\"Fwd IAT Total\",\"Fwd Packet Length Max\",\n",
    "\"Fwd Packet Length Mean\",\"Fwd Packet Length Min\",\"Fwd Packet Length Std\",\"Total Backward Packets\",\"Total Fwd Packets\",\n",
    "\"Total Length of Bwd Packets\",\"Total Length of Fwd Packets\",\"Label\"]}\n",
    "\n",
    "seconds=time.time()#time stamp for all processing time\n",
    "\n",
    "\n",
    "print ('%-17s %-27s  %-15s %-15s %-15s %-15s %-15s' % (\"File\",\"ML algorithm\",\"Accuracy\",\"Precision\", \"Recall\" , \"F1-score\",\"Time\"))# print output header\n",
    "\n",
    "\n",
    "for j in csv_files: #this loop runs on the list containing the filenames.Operations are repeated for all attack files\n",
    "    a=[]\n",
    "    \n",
    "    feature_list=list(features[j[0:-4]])\n",
    "    df=pd.read_csv(path+j,usecols=feature_list)#read an attack file.\n",
    "    df=df.fillna(0)\n",
    "    \n",
    "    attack_or_not=[]\n",
    "    for i in df[\"Label\"]: #it changes the normal label to \"1\" and the attack tag to \"0\" for use in the machine learning algorithm\n",
    "        \n",
    "        if i ==\"BENIGN\":\n",
    "            attack_or_not.append(1)\n",
    "        else:\n",
    "            attack_or_not.append(0)\n",
    "    df[\"Label\"]=attack_or_not\n",
    "\n",
    "    \n",
    "    y = df[\"Label\"] #this section separates the label and the data into two separate pieces, as Label=y Data=X \n",
    "    del df[\"Label\"]\n",
    "    feature_list.remove('Label')\n",
    "    X = df[feature_list]\n",
    "    \n",
    "    precision=[]\n",
    "    recall=[]\n",
    "    f1=[]\n",
    "    accuracy=[]\n",
    "    t_time=[]\n",
    "    \n",
    "    for i in range(repetition): # This loop allows cross-validation and machine learning algorithm to be repeated 10 times\n",
    "        #for i in range(repetition): # This loop allows cross-validation and machine learning algorithm to be repeated 10 times\n",
    "        second=time.time()#time stamp for processing time\n",
    "\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "\n",
    "        # cross-validation\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y,#  data (X) and labels (y) are divided into 2 parts to be sent to the machine learning algorithm (80% train,%20 test). \n",
    "            test_size = 0.20, random_state = repetition)#  So, in total there are 4 tracks: training data(X_train), training tag (y_train), test data(X_test) and test tag(y_test).\n",
    "\n",
    "        #print(\"  Numero de clusters: %d\\n\" % (2))\n",
    "        km = KMeans(init='random', n_clusters=2, random_state=333)\n",
    "        km.fit(X_train)     \n",
    "        predict = km.predict(X_test)\n",
    "\n",
    "        #Para poder identificar qué cluster corresponde\n",
    "        sum0 = sum(1 for p in predict if p == 0)\n",
    "        sum1 = sum(1 for p in predict if p == 1)\n",
    "        if(sum0 > sum1):\n",
    "            predict = predict + 1\n",
    "            predict = np.where(predict == 2, 0, predict)\n",
    "\n",
    "\n",
    "        f_1=f1_score(y_test, predict, average='macro')\n",
    "        pr=precision_score(y_test, predict, average='macro')\n",
    "        rc=recall_score(y_test, predict, average='macro')\n",
    "        score = metrics.accuracy_score(y_test, predict)\n",
    "\n",
    "        precision.append(float(pr))\n",
    "        recall.append(float(rc))\n",
    "        f1.append(float(f_1))\n",
    "        accuracy.append(score)\n",
    "        t_time.append(float((time.time()-second)))\n",
    "\n",
    "    print ('%-17s %-27s  %-15s %-15s %-15s %-15s %-15s' % (j[0:-4],\"K-Means\",str(round(np.mean(accuracy),2)),str(round(np.mean(precision),2)), \n",
    "            str(round(np.mean(recall),2)),str(round(np.mean(f1),2)),str(round(np.mean(t_time),4))))#the result of the ten repetitions is printed on the screen.\n",
    "\n",
    "\n",
    "    \n",
    "print(\"Procesamiento completado\")\n",
    "print(\"Tiempo total de cómputo: = \",time.time()- seconds ,\"segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ALL_DATA: 18 LABELS - K-MEDOIDS\n",
    "\n",
    "# Implementación de los IDS usando 18 parámetros procedentes de cada tipo de ataque\n",
    "# Algoritmo K-Medoids\n",
    "\n",
    "# Para la ejecución de este script se necesita el archivo \"all_data.csv\".\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.cluster import completeness_score\n",
    "\n",
    "#Añadimos nuevos algoritmos de clasificación (no supervisado):\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib as mpl\n",
    "import math\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib as mpl\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler  #Escalar\n",
    "from sklearn.preprocessing import StandardScaler #Estandarizar\n",
    "from sklearn.preprocessing import Normalizer #Normalizar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=50)\n",
    "#%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "import warnings\n",
    "import math\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "csv_files=[\"all_data.csv\"]# CSV files names: #The names of the dataset files (csv_files).\n",
    "path=\"\"\n",
    "\n",
    "repetition=10\n",
    "\n",
    "# Los siguientes 18 parámetros son resultado de combinar los 4 parámetros de mayor importancia para cada tipo de ataque\n",
    "\n",
    "features={\"all_data\":[\"Bwd Packet Length Max\",\"Bwd Packet Length Mean\",\"Bwd Packet Length Std\",\"Flow Bytes/s\",\n",
    "\"Flow Duration\",\"Flow IAT Max\",\"Flow IAT Mean\",\"Flow IAT Min\",\"Flow IAT Std\",\"Fwd IAT Total\",\"Fwd Packet Length Max\",\n",
    "\"Fwd Packet Length Mean\",\"Fwd Packet Length Min\",\"Fwd Packet Length Std\",\"Total Backward Packets\",\"Total Fwd Packets\",\n",
    "\"Total Length of Bwd Packets\",\"Total Length of Fwd Packets\",\"Label\"]}\n",
    "\n",
    "seconds=time.time()#time stamp for all processing time\n",
    "\n",
    "\n",
    "print ('%-17s %-27s  %-15s %-15s %-15s %-15s %-15s' % (\"File\",\"ML algorithm\",\"Accuracy\",\"Precision\", \"Recall\" , \"F1-score\",\"Time\"))# print output header\n",
    "\n",
    "\n",
    "for j in csv_files: #this loop runs on the list containing the filenames.Operations are repeated for all attack files\n",
    "    a=[]\n",
    "    \n",
    "    feature_list=list(features[j[0:-4]])\n",
    "    df=pd.read_csv(path+j,usecols=feature_list)#read an attack file.\n",
    "    df=df.fillna(0)\n",
    "    \n",
    "    attack_or_not=[]\n",
    "    for i in df[\"Label\"]: #it changes the normal label to \"1\" and the attack tag to \"0\" for use in the machine learning algorithm\n",
    "        \n",
    "        if i ==\"BENIGN\":\n",
    "            attack_or_not.append(1)\n",
    "        else:\n",
    "            attack_or_not.append(0)\n",
    "    df[\"Label\"]=attack_or_not\n",
    "\n",
    "    \n",
    "    y = df[\"Label\"] #this section separates the label and the data into two separate pieces, as Label=y Data=X \n",
    "    del df[\"Label\"]\n",
    "    feature_list.remove('Label')\n",
    "    X = df[feature_list]\n",
    "    \n",
    "    precision=[]\n",
    "    recall=[]\n",
    "    f1=[]\n",
    "    accuracy=[]\n",
    "    t_time=[]\n",
    "    \n",
    "    for i in range(repetition): # This loop allows cross-validation and machine learning algorithm to be repeated 10 times\n",
    "        #for i in range(repetition): # This loop allows cross-validation and machine learning algorithm to be repeated 10 times\n",
    "        second=time.time()#time stamp for processing time\n",
    "\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "        \n",
    "        # cross-validation\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y,#  data (X) and labels (y) are divided into 2 parts to be sent to the machine learning algorithm (80% train,%20 test). \n",
    "            test_size = 0.20, random_state = repetition)#  So, in total there are 4 tracks: training data(X_train), training tag (y_train), test data(X_test) and test tag(y_test).\n",
    "\n",
    "        #print(\"  Numero de clusters: %d\\n\" % (2))\n",
    "        km = KMedoids(init='random', n_clusters=2, random_state=333)\n",
    "        km.fit(X_train)     \n",
    "        predict = km.predict(X_test)\n",
    "\n",
    "        #Para poder identificar qué cluster corresponde\n",
    "        sum0 = sum(1 for p in predict if p == 0)\n",
    "        sum1 = sum(1 for p in predict if p == 1)\n",
    "        if(sum0 > sum1):\n",
    "            predict = predict + 1\n",
    "            predict = np.where(predict == 2, 0, predict)\n",
    "\n",
    "\n",
    "        f_1=f1_score(y_test, predict, average='macro')\n",
    "        pr=precision_score(y_test, predict, average='macro')\n",
    "        rc=recall_score(y_test, predict, average='macro')\n",
    "        score = metrics.accuracy_score(y_test, predict)\n",
    "\n",
    "        precision.append(float(pr))\n",
    "        recall.append(float(rc))\n",
    "        f1.append(float(f_1))\n",
    "        accuracy.append(score)\n",
    "        t_time.append(float((time.time()-second)))\n",
    "\n",
    "    print ('%-17s %-27s  %-15s %-15s %-15s %-15s %-15s' % (j[0:-4],\"K-Medoids\",str(round(np.mean(accuracy),2)),str(round(np.mean(precision),2)), \n",
    "            str(round(np.mean(recall),2)),str(round(np.mean(f1),2)),str(round(np.mean(t_time),4))))#the result of the ten repetitions is printed on the screen.\n",
    "\n",
    "    \n",
    "print(\"Procesamiento completado\")\n",
    "print(\"Tiempo total de cómputo: = \",time.time()- seconds ,\"segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ALL_DATA: 18 LABELS - Isolation Forest\n",
    "\n",
    "# Implementación de los IDS usando 18 parámetros procedentes de cada tipo de ataque\n",
    "# Algoritmo Isolation Forest\n",
    "\n",
    "# Para la ejecución de este script se necesita el archivo \"all_data.csv\".\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.cluster import completeness_score\n",
    "\n",
    "#Añadimos nuevos algoritmos de clasificación (no supervisado):\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib as mpl\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler  #Escalar\n",
    "from sklearn.preprocessing import StandardScaler #Estandarizar\n",
    "from sklearn.preprocessing import Normalizer #Normalizar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=50)\n",
    "#%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "import warnings\n",
    "import math\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "csv_files=[\"all_data.csv\"]# CSV files names: #The names of the dataset files (csv_files).\n",
    "path=\"\"\n",
    "\n",
    "repetition=10\n",
    "\n",
    "ml_list={\n",
    "\"Isolation Forest\": IsolationForest(random_state=123, n_estimators = 30, contamination = 0.3, max_samples = \"auto\") #Sabemos que el 30% de los datos corresponden con ataques => contamination=0.3\n",
    "}\n",
    "\n",
    "# Los siguientes 18 parámetros son resultado de combinar los 4 parámetros de mayor importancia para cada tipo de ataque\n",
    "\n",
    "features={\"all_data\":[\"Bwd Packet Length Max\",\"Bwd Packet Length Mean\",\"Bwd Packet Length Std\",\"Flow Bytes/s\",\n",
    "\"Flow Duration\",\"Flow IAT Max\",\"Flow IAT Mean\",\"Flow IAT Min\",\"Flow IAT Std\",\"Fwd IAT Total\",\"Fwd Packet Length Max\",\n",
    "\"Fwd Packet Length Mean\",\"Fwd Packet Length Min\",\"Fwd Packet Length Std\",\"Total Backward Packets\",\"Total Fwd Packets\",\n",
    "\"Total Length of Bwd Packets\",\"Total Length of Fwd Packets\",\"Label\"]}\n",
    "\n",
    "seconds=time.time()#time stamp for all processing time\n",
    "\n",
    "\n",
    "print ('%-17s %-27s  %-15s %-15s %-15s %-15s %-15s' % (\"File\",\"ML algorithm\",\"Accuracy\",\"Precision\", \"Recall\" , \"F1-score\",\"Time\"))# print output header\n",
    "\n",
    "\n",
    "for j in csv_files: #this loop runs on the list containing the filenames.Operations are repeated for all attack files\n",
    "    a=[]\n",
    "    \n",
    "    feature_list=list(features[j[0:-4]])\n",
    "    df=pd.read_csv(path+j,usecols=feature_list)#read an attack file.\n",
    "    df=df.fillna(0)\n",
    "    \n",
    "    attack_or_not=[]\n",
    "    for i in df[\"Label\"]: #it changes the normal label to \"1\" and the attack tag to \"0\" for use in the machine learning algorithm\n",
    "        \n",
    "        if i ==\"BENIGN\":\n",
    "            attack_or_not.append(1)\n",
    "        else:\n",
    "            attack_or_not.append(0)\n",
    "    df[\"Label\"]=attack_or_not\n",
    "\n",
    "    \n",
    "    y = df[\"Label\"] #this section separates the label and the data into two separate pieces, as Label=y Data=X \n",
    "    del df[\"Label\"]\n",
    "    feature_list.remove('Label')\n",
    "    X = df[feature_list]\n",
    "    \n",
    "    for ii in ml_list:\n",
    "        precision=[]\n",
    "        recall=[]\n",
    "        f1=[]\n",
    "        accuracy=[]\n",
    "        t_time=[]\n",
    "        \n",
    "        for i in range(repetition): # This loop allows cross-validation and machine learning algorithm to be repeated 10 times\n",
    "            #for i in range(repetition): # This loop allows cross-validation and machine learning algorithm to be repeated 10 times\n",
    "            second=time.time()#time stamp for processing time\n",
    "\n",
    "            # cross-validation\n",
    "            scaler = StandardScaler()\n",
    "            X = scaler.fit_transform(X)\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y,#  data (X) and labels (y) are divided into 2 parts to be sent to the machine learning algorithm (80% train,%20 test). \n",
    "                test_size = 0.20, random_state = repetition)#  So, in total there are 4 tracks: training data(X_train), training tag (y_train), test data(X_test) and test tag(y_test).\n",
    "\n",
    "\n",
    "            clf = ml_list[ii]#choose algorithm from ml_list dictionary                                                                          \n",
    "            clf.fit(X_train)\n",
    "            predict =clf.predict(X_test)\n",
    "\n",
    "            predict = np.where(predict == -1, 0, predict) #El resultado de isolation forest son 1 para los normales y -1 para anomalías\n",
    "\n",
    "            #makes \"classification report\" and assigns the precision, f-measure, and recall values.s.    \n",
    "            f_1=f1_score(y_test, predict, average='macro')\n",
    "            pr=precision_score(y_test, predict, average='macro')\n",
    "            rc=recall_score(y_test, predict, average='macro')\n",
    "            score = metrics.accuracy_score(y_test, predict)\n",
    "\n",
    "            precision.append(float(pr))\n",
    "            recall.append(float(rc))\n",
    "            f1.append(float(f_1))\n",
    "            accuracy.append(score)\n",
    "            t_time.append(float((time.time()-second)))\n",
    "\n",
    "        print ('%-17s %-27s  %-15s %-15s %-15s %-15s %-15s' % (j[0:-4],ii,str(round(np.mean(accuracy),2)),str(round(np.mean(precision),2)), \n",
    "                str(round(np.mean(recall),2)),str(round(np.mean(f1),2)),str(round(np.mean(t_time),4))))#the result of the ten repetitions is printed on the screen.\n",
    "\n",
    "\n",
    "    \n",
    "print(\"Procesamiento completado\")\n",
    "print(\"Tiempo total de cómputo: = \",time.time()- seconds ,\"segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ALL_DATA: 18 LABELS - Local Outlier Factor\n",
    "\n",
    "# Implementación de los IDS usando 18 parámetros procedentes de cada tipo de ataque\n",
    "# Algoritmo Local Outlier Factor\n",
    "\n",
    "# Para la ejecución de este script se necesita el archivo \"all_data.csv\".\n",
    "\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.cluster import completeness_score\n",
    "\n",
    "#Añadimos nuevos algoritmos de clasificación (no supervisado):\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib as mpl\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler  #Escalar\n",
    "from sklearn.preprocessing import StandardScaler #Estandarizar\n",
    "from sklearn.preprocessing import Normalizer #Normalizar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=50)\n",
    "#%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "import warnings\n",
    "import math\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "csv_files=[\"all_data.csv\"]# CSV files names: #The names of the dataset files (csv_files).\n",
    "path=\"\"\n",
    "\n",
    "repetition=10\n",
    "\n",
    "ml_list={\n",
    "\"LOF\": LocalOutlierFactor(n_neighbors=25, contamination=0.2) #Sabemos que el 30% de los datos corresponden con ataques => contamination=0.3\n",
    "}\n",
    "\n",
    "# Los siguientes 18 parámetros son resultado de combinar los 4 parámetros de mayor importancia para cada tipo de ataque\n",
    "\n",
    "features={\"all_data\":[\"Bwd Packet Length Max\",\"Bwd Packet Length Mean\",\"Bwd Packet Length Std\",\"Flow Bytes/s\",\n",
    "\"Flow Duration\",\"Flow IAT Max\",\"Flow IAT Mean\",\"Flow IAT Min\",\"Flow IAT Std\",\"Fwd IAT Total\",\"Fwd Packet Length Max\",\n",
    "\"Fwd Packet Length Mean\",\"Fwd Packet Length Min\",\"Fwd Packet Length Std\",\"Total Backward Packets\",\"Total Fwd Packets\",\n",
    "\"Total Length of Bwd Packets\",\"Total Length of Fwd Packets\",\"Label\"]}\n",
    "\n",
    "seconds=time.time()#time stamp for all processing time\n",
    "\n",
    "\n",
    "print ('%-17s %-27s  %-15s %-15s %-15s %-15s %-15s' % (\"File\",\"ML algorithm\",\"Accuracy\",\"Precision\", \"Recall\" , \"F1-score\",\"Time\"))# print output header\n",
    "\n",
    "\n",
    "for j in csv_files: #this loop runs on the list containing the filenames.Operations are repeated for all attack files\n",
    "    a=[]\n",
    "    \n",
    "    feature_list=list(features[j[0:-4]])\n",
    "    df=pd.read_csv(path+j,usecols=feature_list)#read an attack file.\n",
    "    df=df.fillna(0)\n",
    "    \n",
    "    attack_or_not=[]\n",
    "    for i in df[\"Label\"]: #it changes the normal label to \"1\" and the attack tag to \"0\" for use in the machine learning algorithm\n",
    "        \n",
    "        if i ==\"BENIGN\":\n",
    "            attack_or_not.append(1)\n",
    "        else:\n",
    "            attack_or_not.append(0)\n",
    "    df[\"Label\"]=attack_or_not\n",
    "\n",
    "    \n",
    "    y = df[\"Label\"] #this section separates the label and the data into two separate pieces, as Label=y Data=X \n",
    "    del df[\"Label\"]\n",
    "    feature_list.remove('Label')\n",
    "    X = df[feature_list]\n",
    "    \n",
    "\n",
    "    for ii in ml_list:\n",
    "        precision=[]\n",
    "        recall=[]\n",
    "        f1=[]\n",
    "        accuracy=[]\n",
    "        t_time=[]\n",
    "        \n",
    "        for i in range(repetition): # This loop allows cross-validation and machine learning algorithm to be repeated 10 times\n",
    "            #for i in range(repetition): # This loop allows cross-validation and machine learning algorithm to be repeated 10 times\n",
    "            second=time.time()#time stamp for processing time\n",
    "\n",
    "            # cross-validation\n",
    "            scaler = StandardScaler()\n",
    "            X = scaler.fit_transform(X)\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y,#  data (X) and labels (y) are divided into 2 parts to be sent to the machine learning algorithm (80% train,%20 test). \n",
    "                test_size = 0.20, random_state = repetition)#  So, in total there are 4 tracks: training data(X_train), training tag (y_train), test data(X_test) and test tag(y_test).\n",
    "\n",
    "            clf = ml_list[ii]#choose algorithm from ml_list dictionary                                                                          \n",
    "            clf.fit(X_train)\n",
    "            predict =clf.fit_predict(X_test)\n",
    "\n",
    "            predict = np.where(predict == -1, 0, predict) #El resultado de LOF son 1 para los normales y -1 para anomalías\n",
    "\n",
    "            #makes \"classification report\" and assigns the precision, f-measure, and recall values.s.    \n",
    "            f_1=f1_score(y_test, predict, average='macro')\n",
    "            pr=precision_score(y_test, predict, average='macro')\n",
    "            rc=recall_score(y_test, predict, average='macro')\n",
    "            score = metrics.accuracy_score(y_test, predict)\n",
    "\n",
    "            precision.append(float(pr))\n",
    "            recall.append(float(rc))\n",
    "            f1.append(float(f_1))\n",
    "            accuracy.append(score)\n",
    "            t_time.append(float((time.time()-second)))\n",
    "\n",
    "        print ('%-17s %-27s  %-15s %-15s %-15s %-15s %-15s' % (j[0:-4],ii,str(round(np.mean(accuracy),2)),str(round(np.mean(precision),2)), \n",
    "                str(round(np.mean(recall),2)),str(round(np.mean(f1),2)),str(round(np.mean(t_time),4))))#the result of the ten repetitions is printed on the screen\n",
    "\n",
    "\n",
    "    \n",
    "print(\"Procesamiento completado\")\n",
    "print(\"Tiempo total de cómputo: = \",time.time()- seconds ,\"segundos\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
